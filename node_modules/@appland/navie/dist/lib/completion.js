"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __await = (this && this.__await) || function (v) { return this instanceof __await ? (this.v = v, this) : new __await(v); }
var __asyncValues = (this && this.__asyncValues) || function (o) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var m = o[Symbol.asyncIterator], i;
    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);
    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }
    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }
};
var __asyncGenerator = (this && this.__asyncGenerator) || function (thisArg, _arguments, generator) {
    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");
    var g = generator.apply(thisArg, _arguments || []), i, q = [];
    return i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i;
    function verb(n) { if (g[n]) i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; }
    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }
    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }
    function fulfill(value) { resume("next", value); }
    function reject(value) { resume("throw", value); }
    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }
};
Object.defineProperty(exports, "__esModule", { value: true });
const node_console_1 = require("node:console");
const messages_1 = require("@langchain/core/messages");
const MESSAGE_CONSTRUCTORS = {
    assistant: messages_1.AIMessage,
    system: messages_1.SystemMessage,
    user: messages_1.HumanMessage,
    tool: messages_1.SystemMessage,
    function: messages_1.SystemMessage,
};
function convertToMessage(message) {
    var _a;
    const Cons = MESSAGE_CONSTRUCTORS[message.role];
    return new Cons({ content: (_a = message.content) !== null && _a !== void 0 ? _a : '' });
}
/*
  Generated on https://openai.com/api/pricing/ with
  Object.fromEntries(
    $$('.w-full')
      .filter(
        ({ children: [c] }) => [...(c?.classList ?? [])]?.includes('grid') && c.children.length === 3
      )
      .map(({ children: [, ...items] }) => items)
      .flat()
      .map(({ children: [...c] }) => c.map((x) => x.innerText))
      .map(([model, input, output]) => [
        model,
        { input: parseFloat(input.replace(',', '.')), output: parseFloat(input.replace(',', '.')) },
      ])
  );
*/
const COST_PER_M_TOKEN = {
    'gpt-4o': {
        input: 5,
        output: 5,
    },
    'gpt-4o-2024-05-13': {
        input: 5,
        output: 5,
    },
    'gpt-3.5-turbo-0125': {
        input: 0.5,
        output: 0.5,
    },
    'gpt-3.5-turbo-instruct': {
        input: 1.5,
        output: 1.5,
    },
    'gpt-4-turbo': {
        input: 10,
        output: 10,
    },
    'gpt-4-turbo-2024-04-09': {
        input: 10,
        output: 10,
    },
    'gpt-4': {
        input: 30,
        output: 30,
    },
    'gpt-4-32k': {
        input: 60,
        output: 60,
    },
    'gpt-4-0125-preview': {
        input: 10,
        output: 10,
    },
    'gpt-4-1106-preview': {
        input: 10,
        output: 10,
    },
    'gpt-4-vision-preview': {
        input: 10,
        output: 10,
    },
    'gpt-3.5-turbo-1106': {
        input: 1,
        output: 1,
    },
    'gpt-3.5-turbo-0613': {
        input: 1.5,
        output: 1.5,
    },
    'gpt-3.5-turbo-16k-0613': {
        input: 3,
        output: 3,
    },
    'gpt-3.5-turbo-0301': {
        input: 1.5,
        output: 1.5,
    },
    'davinci-002': {
        input: 2,
        output: 2,
    },
    'babbage-002': {
        input: 0.4,
        output: 0.4,
    },
};
function resultToString() {
    let result = `Tokens (prompt/compl/total): ${this.promptTokens}/${this.completionTokens}/${this.totalTokens}`;
    if (this.cost)
        result += `, cost: $${this.cost.toFixed(2)}`;
    return result;
}
function tokenUsage(promptTokens, completionTokens, modelName) {
    const result = {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens,
    };
    const prices = COST_PER_M_TOKEN[modelName];
    if (prices)
        result.cost = (prices.input * promptTokens + prices.output * completionTokens) / 1000000;
    result.toString = resultToString;
    return result;
}
function estimateTokens(messages) {
    const nonEmpty = messages.map((x) => { var _a, _b; return (_b = (_a = x.content) === null || _a === void 0 ? void 0 : _a.toString().length) !== null && _b !== void 0 ? _b : 0; });
    if (nonEmpty.length)
        return nonEmpty.reduce((x, y) => x + y);
    return 0;
}
function countTokens(openAI, messages) {
    var _a;
    return __awaiter(this, void 0, void 0, function* () {
        try {
            const count = yield openAI.getNumTokensFromMessages(messages.map(convertToMessage));
            return (_a = count === null || count === void 0 ? void 0 : count.totalCount) !== null && _a !== void 0 ? _a : estimateTokens(messages);
        }
        catch (e) {
            return estimateTokens(messages);
        }
    });
}
// Some LLMs only accept a single system message.
// This functions merges all system messages into a single message
// at the start of the list.
function mergeSystemMessages(messages) {
    const systemMessages = messages.filter((message) => message.role === 'system');
    const nonSystemMessages = messages.filter((message) => message.role !== 'system');
    const mergedSystemMessage = {
        role: 'system',
        content: systemMessages.map((message) => message.content).join('\n'),
    };
    return [mergedSystemMessage, ...nonSystemMessages];
}
function completion(openAI, messages) {
    return __asyncGenerator(this, arguments, function* completion_1() {
        var _a, e_1, _b, _c;
        const promptTokensPromise = countTokens(openAI, messages);
        const response = yield __await(openAI.completionWithRetry({
            messages: mergeSystemMessages(messages),
            model: openAI.modelName,
            stream: true,
        }));
        let tokenCount = 0;
        try {
            for (var _d = true, response_1 = __asyncValues(response), response_1_1; response_1_1 = yield __await(response_1.next()), _a = response_1_1.done, !_a;) {
                _c = response_1_1.value;
                _d = false;
                try {
                    const token = _c;
                    const { content } = token.choices[0].delta;
                    if (content)
                        yield yield __await(content);
                    tokenCount += 1;
                }
                finally {
                    _d = true;
                }
            }
        }
        catch (e_1_1) { e_1 = { error: e_1_1 }; }
        finally {
            try {
                if (!_d && !_a && (_b = response_1.return)) yield __await(_b.call(response_1));
            }
            finally { if (e_1) throw e_1.error; }
        }
        const usage = tokenUsage(yield __await(promptTokensPromise), tokenCount, openAI.modelName);
        (0, node_console_1.warn)(usage.toString());
        return yield __await(usage);
    });
}
exports.default = completion;
